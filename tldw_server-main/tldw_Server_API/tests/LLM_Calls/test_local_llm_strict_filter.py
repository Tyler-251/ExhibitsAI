import json
from unittest.mock import MagicMock, patch

import pytest

from tldw_Server_API.app.core.Chat.chat_orchestrator import chat_api_call


class DummyResponse:
    def __init__(self, payload: dict):
        self._payload = payload
        self.status_code = 200

    def raise_for_status(self):
        return None

    def json(self):
        return {}

    def close(self):
        return None


@pytest.mark.unit
@pytest.mark.strict_mode
def test_local_llm_strict_filter_drops_top_k_from_payload_non_streaming():
    # local_llm strict mode configuration
    fake_settings = {
        "local_llm": {
            "api_ip": "http://localhost:8080/v1/chat/completions",
            "streaming": False,
            "strict_openai_compat": True,
        }
    }

    captured_payload = {}

    def fake_post(url, headers=None, json=None, timeout=None):
        # capture outgoing payload
        captured_payload.clear()
        if json:
            captured_payload.update(json)
        return DummyResponse({})

    with patch(
        "tldw_Server_API.app.core.LLM_Calls.LLM_API_Calls_Local.load_settings",
        return_value=fake_settings,
    ), patch(
        "tldw_Server_API.app.core.LLM_Calls.LLM_API_Calls_Local.httpx.Client"
    ) as mock_client_cls:
        mock_client = MagicMock()
        mock_client.post.side_effect = fake_post
        mock_client.close.return_value = None
        mock_client_cls.return_value = mock_client

        # Invoke via chat_api_call so provider mapping applies (topk -> top_k)
        chat_api_call(
            api_endpoint="local-llm",
            api_key=None,
            messages_payload=[{"role": "user", "content": "hello"}],
            topk=23,  # non-standard for strict OpenAI-compatible servers
            streaming=False,
        )

    # In strict mode, top_k should not be in the posted payload
    assert "top_k" not in captured_payload
    # Sanity check: standard keys remain
    assert "messages" in captured_payload
    assert "stream" in captured_payload
