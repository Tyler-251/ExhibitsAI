# Prompt Studio - ProgramEvaluator (Sandboxed Code Evaluation) Design

- Version: v0.1 (Design Stub)
- Owner: Prompt Studio
- Status: Draft (Phase 2 follow-up to MCTS PRD)

## Purpose

Provide an optional, sandboxed execution environment to evaluate code generated by prompt sequences (e.g., for optimization tasks). Map execution outcomes to a numeric reward (-1..10) usable by MCTS and other optimizers. Disabled by default; enabled via feature flag/config.

## Scope (Phase 2)

- Add a `ProgramEvaluator` component to safely run user-generated Python snippets with strict limits and no external side effects.
- Integrate with `TestRunner` to support test cases of type `program` (e.g., runner = "python").
- Return structured result: {success, stdout, stderr, metrics, reward}.

## Safety Requirements

- Isolation: run in a jailed interpreter or subprocess with:
  - CPU time limit (e.g., 2-5s per run), wall clock timeout.
  - Memory limit (e.g., 128-512MB).
  - No network, no file system writes/reads by default.
  - Restricted builtins and import whitelist (e.g., `math`, `statistics`, `numpy` optional).
- Logging: redact inputs; don’t store code content beyond necessary metadata.
- Config flags: `PROMPT_STUDIO_ENABLE_CODE_EVAL=false` by default; per-project override.

## API Contract (Internal)

`ProgramEvaluator.evaluate(code: str, context: dict) -> dict`

- Input:
  - `code`: Python code string to execute.
  - `context`: optional parameters (inputs, objective, constraints, scoring policy).
- Output:
  - `success`: bool
  - `return_code`: int
  - `stdout`: str (truncated)
  - `stderr`: str (truncated)
  - `metrics`: dict (e.g., objective value, constraint satisfied flags)
  - `reward`: float in [-1, 10]

## Reward Policy (Initial)

- Execution failure (timeout, syntax/runtime error) → `reward = -1`.
- Success with metrics available:
  - Objective optimized and constraints satisfied → scale to [7..10].
  - Partial satisfaction (some constraints) → [3..6].
  - Executed but invalid results → 0..2.

## Integration Points

- `TestRunner.run_single_test`:
  - If test case `runner == 'python'` and feature flag enabled, delegate to `ProgramEvaluator`.
  - Map reward to existing aggregate score (normalize 0..10 → 0..1) when required.
- MCTS optimizer:
  - Use evaluator-derived reward for simulation rollouts when test cases are programmatic.
- Storage:
  - Persist only summary metadata (reward, timings). Full stdout/stderr optional & truncated.

## Configuration

- Env / config.txt:
  - `PROMPT_STUDIO_ENABLE_CODE_EVAL=false`
  - `PROMPT_STUDIO_CODE_EVAL_TIMEOUT_MS=3000`
  - `PROMPT_STUDIO_CODE_EVAL_MEM_MB=256`
  - `PROMPT_STUDIO_CODE_EVAL_IMPORT_WHITELIST=math,statistics`

## Open Questions

- Use OS-level sandbox (seccomp/ptrace/Firejail) vs Python-only constraints?
- Support numeric libs (numpy, cvxpy) safely? Likely opt-in per project with stricter limits.
- Cross-platform consistency (Linux/macOS/Windows) for local dev.

## Milestones

- M2.1: Minimal evaluator (subprocess, time/mem limit, no network, import gate).
- M2.2: Integrate with TestRunner and MCTS reward path; add metrics & logs.
- M2.3: Hardening (deny-list syscalls, resource enforcement), documentation & examples.
