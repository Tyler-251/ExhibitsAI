# Diarization


https://github.com/narcotic-sh/zanshin
https://github.com/narcotic-sh/senko
https://github.com/AugmendTech/treeseg?tab=readme-ov-file


Implementing speaker diarization using vector embeddings is an excellent, modern approach that avoids the complexities of traditional methods and the specific dependencies of libraries like `pyannote`. This technique hinges on creating a unique "voice fingerprint" (a vector embedding) for each speech segment and then clustering these fingerprints to group segments from the same speaker.

Here is a comprehensive guide on how to implement this, broken down into a high-level conceptual flow and the practical steps involved.

### High-Level Flow

The process can be broken down into four main stages:

1.  **Voice Activity Detection (VAD):** First, you need to distinguish between speech and silence in the audio. This is crucial to avoid generating embeddings for non-speech segments, which would add noise to the clustering process.
2.  **Speech Segmentation:** Once you have identified the speech portions, you need to break them down into smaller, manageable chunks. These segments should be long enough to contain sufficient speaker-specific information but short enough to likely contain only one speaker.
3.  **Embedding Extraction:** For each speech segment, you will use a pre-trained speaker embedding model to generate a fixed-size vector (the "voice fingerprint"). This vector numerically represents the unique characteristics of the speaker's voice in that segment.
4.  **Clustering and Refinement:** Finally, you will use a clustering algorithm to group the generated embeddings. Each resulting cluster will represent a unique speaker. The segment timestamps associated with the embeddings in a cluster can then be merged to create the final diarization output.

---

### Step-by-Step Implementation Guide

Hereâ€™s how you can build this pipeline:

#### Step 1: Voice Activity Detection (VAD)

The goal is to get a series of timestamps indicating when speech occurs.

*   **Recommended Tool:** The `silero-vad` library is an excellent, lightweight, and highly accurate choice. It's much simpler to use than the VAD components of larger libraries.

*   **How it works:** You feed your raw audio into the Silero VAD model, and it will output timestamps for detected speech.

    **Conceptual Code:**
    ```python
    import torch
    import torchaudio

    # Load the Silero VAD model
    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                  model='silero_vad',
                                  force_reload=True)

    (get_speech_timestamps,
     save_audio,
     read_audio,
     VADIterator,
     collect_chunks) = utils

    # Load your audio file (resample to 16kHz as required by the model)
    wav = read_audio('your_audio.wav', sampling_rate=16000)

    # Get speech timestamps
    speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=16000)
    print("Speech timestamps:", speech_timestamps)
    ```

#### Step 2: Speech Segmentation

Now, you need to create audio segments based on the speech timestamps from the VAD. A common approach is to create fixed-length segments (e.g., 1.5 to 2.5 seconds) with some overlap (e.g., 0.5 seconds).

*   **Why overlap?** Overlapping segments ensure that you get clean embeddings even if a speaker's turn starts or ends in the middle of a segment.
*   **Implementation:** You can iterate through the audio using the VAD timestamps to create these segments.

    **Conceptual Code:**
    ```python
    def create_segments(audio_waveform, segment_length_s=2.0, overlap_s=0.5):
        segments = []
        sample_rate = 16000  # Assuming 16kHz audio
        segment_length_samples = int(segment_length_s * sample_rate)
        overlap_samples = int(overlap_s * sample_rate)
        step = segment_length_samples - overlap_samples

        for i in range(0, len(audio_waveform) - segment_length_samples, step):
            start = i
            end = i + segment_length_samples
            segments.append({
                "start_time": start / sample_rate,
                "end_time": end / sample_rate,
                "waveform": audio_waveform[start:end]
            })
        return segments

    # This would be applied to the speech parts of the audio
    speech_segments = create_segments(wav)
    ```

#### Step 3: Embedding Extraction

This is the core of the process. You'll use a pre-trained model to convert each audio segment into a vector embedding.

*   **Recommended Models/Libraries:**
    *   **SpeechBrain:** A powerful and flexible PyTorch-based toolkit. It offers excellent pre-trained models for speaker verification, which are perfect for generating embeddings. Their `spkrec-ecapa-voxceleb` model is a popular choice.
    *   **NVIDIA NeMo:** Another fantastic toolkit with state-of-the-art models for speaker recognition, like `titanet` or `speakerverification_speakernet`.
    *   **Resemblyzer:** A simpler, high-level library that is great for getting started, though it may offer less control than SpeechBrain or NeMo.

*   **How it works:** You load the pre-trained model and pass each audio segment through it to get the embedding.

    **Conceptual Code (using SpeechBrain):**
    ```python
    from speechbrain.pretrained import EncoderClassifier

    # Load a pre-trained speaker embedding model
    classifier = EncoderClassifier.from_hparams(source="speechbrain/spkrec-ecapa-voxceleb")

    embeddings = []
    for segment in speech_segments:
        # The model expects audio in a specific format (torch.Tensor)
        waveform = torch.tensor(segment["waveform"]).unsqueeze(0)

        # Generate the embedding
        embedding = classifier.encode_batch(waveform)

        # Squeeze to remove unnecessary dimensions and convert to numpy
        embedding = embedding.squeeze().numpy()

        embeddings.append(embedding)
        segment['embedding'] = embedding # Store embedding with its segment info
    ```

#### Step 4: Clustering Embeddings

You now have a list of vector embeddings, where each vector represents a voice fingerprint. The task is to group these vectors so that each group corresponds to a single speaker.

*   **Important Consideration:** You often don't know the number of speakers in advance. Therefore, clustering algorithms that don't require a pre-defined number of clusters (`k`) are often preferred.

*   **Recommended Clustering Algorithms:**
    *   **Agglomerative Hierarchical Clustering:** This is a common choice. It starts by treating each embedding as its own cluster and then iteratively merges the closest clusters until a stopping condition (like a distance threshold) is met.
    *   **Spectral Clustering:** This method works well for this task. You can use it by constructing a similarity matrix from your embeddings and then running the algorithm. It can also estimate the number of clusters (speakers).
    *   **K-Means Clustering:** If you *do* know the number of speakers, K-Means is a fast and effective option.

*   **How it works (using Scikit-learn for Spectral Clustering):**
    1.  **Normalize Embeddings:** It's a good practice to normalize the embeddings before clustering.
    2.  **Affinity Matrix:** Calculate the cosine similarity between all pairs of embeddings. This creates a matrix where high values indicate that two segments are from the same speaker.
    3.  **Clustering:** Apply the clustering algorithm to this matrix.

    **Conceptual Code:**
    ```python
    import numpy as np
    from sklearn.cluster import SpectralClustering
    from sklearn.preprocessing import normalize

    # Convert list of embeddings to a numpy array
    embedding_array = np.array(embeddings)

    # Normalize the embeddings
    embedding_array = normalize(embedding_array, axis=1, norm='l2')

    # You can try to estimate the number of speakers or set it if you know it
    # For estimation, you can use methods like the elbow method or silhouette score
    # Or, let the algorithm decide if possible.

    # Let's assume we want to find 2 speakers
    num_speakers = 2

    clustering = SpectralClustering(n_clusters=num_speakers,
                                    assign_labels='kmeans',
                                    random_state=0).fit(embedding_array)

    # The `clustering.labels_` array now holds the speaker ID for each segment
    for i, segment in enumerate(speech_segments):
        segment['speaker_id'] = clustering.labels_[i]
    ```

#### Step 5: Final Diarization and Merging

You now have a list of segments, each with a start time, end time, and a speaker ID. The final step is to merge consecutive segments from the same speaker.

**Conceptual Code:**
```python
def merge_segments(segments):
    if not segments:
        return []

    merged = []
    current_speaker = segments[0]['speaker_id']
    current_start = segments[0]['start_time']
    current_end = segments[0]['end_time']

    for i in range(1, len(segments)):
        segment = segments[i]
        if segment['speaker_id'] == current_speaker and segment['start_time'] < current_end + 0.5: # Merge if same speaker and close in time
            current_end = segment['end_time']
        else:
            merged.append({'speaker': f"Speaker_{current_speaker}", 'start': current_start, 'end': current_end})
            current_speaker = segment['speaker_id']
            current_start = segment['start_time']
            current_end = segment['end_time']

    merged.append({'speaker': f"Speaker_{current_speaker}", 'start': current_start, 'end': current_end})
    return merged

final_diarization = merge_segments(speech_segments)
print(final_diarization)

# Expected Output:
# [{'speaker': 'Speaker_0', 'start': 0.5, 'end': 4.2},
#  {'speaker': 'Speaker_1', 'start': 4.8, 'end': 8.1},
#  {'speaker': 'Speaker_0', 'start': 8.5, 'end': 12.0}]
```

By following these steps, you can build a powerful and flexible speaker diarization pipeline from the ground up, giving you full control over each component of the process.
